# Document Manager Environment Configuration
# Copy this file to .env and customize as needed

# ============================================
# Database Configuration
# ============================================
DB_USER=postgres
DB_PASSWORD=password
DB_NAME=archive_brain

# ============================================
# LLM Configuration
# ============================================
# Default: Uses containerized Ollama (recommended for SaaS/portability)
# To use external Ollama (e.g., host with GPU): OLLAMA_URL=http://host.docker.internal:11434
OLLAMA_URL=http://ollama:11434

# Model selection
OLLAMA_MODEL=dolphin-phi
OLLAMA_EMBEDDING_MODEL=nomic-embed-text
OLLAMA_VISION_MODEL=llava

# Set to true for development without LLM (returns mock responses)
OLLAMA_MOCK=false

# ============================================
# Future: Cloud LLM Providers (SaaS mode)
# ============================================
# LLM_PROVIDER=openai  # Options: ollama, openai, anthropic, azure
# OPENAI_API_KEY=sk-...
# OPENAI_MODEL=gpt-4o-mini
# OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# ANTHROPIC_API_KEY=sk-ant-...
# ANTHROPIC_MODEL=claude-3-haiku-20240307

# ============================================
# Archive Paths (adjust for your setup)
# ============================================
# These control optional Docker bind mounts for your local content.
# They should point to folders on the *host* running Docker (WSL paths are fine when using Docker-in-WSL).
# If unset, Compose defaults to local folders under ./archive_root (prod) or ./archive_dev (dev).

# STORY_SOURCE=/mnt/c/Users/<you>/Documents/story/authors
# KNOWLEDGE_SOURCE=/mnt/c/Users/<you>/Documents/knowledge