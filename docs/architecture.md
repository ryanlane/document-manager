# Architecture Overview

This document explains how Archive Brain is structured and how data flows through the system.

---

## ğŸ§© High-Level Components

- **Frontend** â€“ React UI for search and browsing
- **API** â€“ FastAPI service for queries and RAG
- **Worker** â€“ Background ingestion and enrichment pipeline
- **Database** â€“ PostgreSQL + pgvector
- **LLM Runtime** â€“ Ollama (local or external)

All components communicate over Docker networks.

---

## ğŸ” Data Flow

1. **Ingestion**
   - Files scanned from configured directories
   - Content extracted via Tika / OCR

2. **Segmentation**
   - Large files split into logical chunks
   - Chunk boundaries stored in the database

3. **Enrichment**
   - LLM generates metadata:
     - Title
     - Summary
     - Tags

4. **Embedding**
   - Each chunk converted into a vector embedding
   - Stored in pgvector for semantic search

5. **Retrieval & Generation**
   - Search queries retrieve relevant chunks
   - LLM synthesizes responses from retrieved context

---

## ğŸ—„ï¸ Database Model (Conceptual)

- **Files** â€“ Original documents
- **Entries** â€“ Segmented chunks
- **Metadata** â€“ LLM-generated enrichment
- **Embeddings** â€“ Vector representations

The database is the systemâ€™s source of truth.

---

## ğŸ§  Model Roles

- **Chat model** â€“ Summaries, Q&A
- **Embedding model** â€“ Semantic similarity
- **Vision model** â€“ Image descriptions

Models are interchangeable via environment variables.

---

## âš™ï¸ Extensibility

Advanced users can customize:
- Chunking strategy
- Metadata schema
- Ranking and retrieval logic
- UI presentation

Most extensions live in the worker pipeline.

---

## ğŸ§­ Design Philosophy

- Local-first
- Transparent processing
- Inspectable data
- Composable architecture

Archive Brain favors **clarity over cleverness**.