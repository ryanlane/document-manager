services:
  db:
    image: pgvector/pgvector:pg16
    environment:
      POSTGRES_USER: ${DB_USER:-postgres}
      POSTGRES_PASSWORD: ${DB_PASSWORD:-password}
      POSTGRES_DB: ${DB_NAME:-archive_brain}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    networks:
      - archive_net
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 8G
        reservations:
          cpus: '1'
          memory: 1G

  tika:
    image: apache/tika:latest
    ports:
      - "9998:9998"
    networks:
      - archive_net
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 4G
        reservations:
          cpus: '1'
          memory: 512M

  # Ollama LLM service - self-contained inference with GPU support
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_models:/root/.ollama
    networks:
      - archive_net
    healthcheck:
      test: ["CMD-SHELL", "ollama list || exit 1"]
      interval: 30s
      timeout: 30s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Model initialization - pulls required models on first run
  ollama-init:
    image: ollama/ollama:latest
    depends_on:
      ollama:
        condition: service_healthy
    networks:
      - archive_net
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        echo "Pulling required models..."
        ollama pull nomic-embed-text
        ollama pull qwen2:1.5b
        ollama pull llava
        echo "Models ready!"
    environment:
      - OLLAMA_HOST=ollama:11434
    restart: "no"

  worker:
    build: ./backend
    volumes:
      - ./backend:/app
      - ./config:/app/config
      # Mount the data directories here so the worker can access them
      - ./archive_root:/data/archive
      - /mnt/mediaboy/story:/data/archive/story
      - worker_shared:/app/shared
    environment:
      - DB_HOST=db
      - DB_PORT=5432
      - DB_USER=${DB_USER:-postgres}
      - DB_PASSWORD=${DB_PASSWORD:-password}
      - DB_NAME=${DB_NAME:-archive_brain}
      - TIKA_URL=http://tika:9998
      # Use containerized Ollama by default, can override with external URL
      - OLLAMA_URL=${OLLAMA_URL:-http://ollama:11434}
      - OLLAMA_MOCK=${OLLAMA_MOCK:-false}
      - OLLAMA_MODEL=${OLLAMA_MODEL:-qwen2:1.5b}
      - OLLAMA_EMBEDDING_MODEL=${OLLAMA_EMBEDDING_MODEL:-nomic-embed-text}
      - OLLAMA_VISION_MODEL=${OLLAMA_VISION_MODEL:-llava}
    depends_on:
      ollama:
        condition: service_healthy
      db:
        condition: service_started
      tika:
        condition: service_started
    command: python -m src.worker_loop
    networks:
      - archive_net
    # Resource allocation - worker gets most resources for heavy processing
    deploy:
      resources:
        limits:
          cpus: '8'
          memory: 16G
        reservations:
          cpus: '2'
          memory: 2G

  api:
    build: ./backend
    ports:
      - "8000:8000"
    volumes:
      - ./backend:/app
      - ./config:/app/config
      - ./archive_root:/data/archive
      # Mount the network drive (ensure this path exists and is mounted in WSL)
      - /mnt/mediaboy/story:/data/archive/story
      - worker_shared:/app/shared
    environment:
      - DB_HOST=db
      - DB_PORT=5432
      - DB_USER=${DB_USER:-postgres}
      - DB_PASSWORD=${DB_PASSWORD:-password}
      - DB_NAME=${DB_NAME:-archive_brain}
      # Use containerized Ollama by default, can override with external URL
      - OLLAMA_URL=${OLLAMA_URL:-http://ollama:11434}
      - OLLAMA_MOCK=${OLLAMA_MOCK:-false}
      - OLLAMA_MODEL=${OLLAMA_MODEL:-qwen2:1.5b}
      - OLLAMA_EMBEDDING_MODEL=${OLLAMA_EMBEDDING_MODEL:-nomic-embed-text}
      - OLLAMA_VISION_MODEL=${OLLAMA_VISION_MODEL:-llava}
    depends_on:
      ollama:
        condition: service_healthy
      db:
        condition: service_started
    # Use multiple workers for better concurrency, watchfiles for hot reload
    command: uvicorn src.api.main:app --host 0.0.0.0 --port 8000 --workers 4 --timeout-keep-alive 30
    networks:
      - archive_net
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 4G
        reservations:
          cpus: '1'
          memory: 512M

  frontend:
    build: ./frontend
    ports:
      - "3000:3000"
    volumes:
      - ./frontend:/app
      - /app/node_modules
    depends_on:
      - api
    networks:
      - archive_net

volumes:
  postgres_data:
  worker_shared:
  ollama_models:

networks:
  archive_net:
